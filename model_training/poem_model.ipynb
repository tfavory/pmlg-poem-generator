{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perth Machine Learning Group Poem Generator\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The following code uses GRU to generate poems. It reads through a corpus of poems, and learns sequences of characters, including line breaks and titles.\n",
    "\n",
    "In short, it observes many sequences of characters, and infers the character that should come next. For instance, it guesses that after 'The cat eat' should come the letter 's'.\n",
    "\n",
    "Further details will be given with the code.\n",
    "\n",
    "## The code\n",
    "\n",
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf  # version 1.9 or above\n",
    "tf.enable_eager_execution()  # Execution of code as it runs in the notebook. Normally, TensorFlow looks up the whole code before execution for efficiency.\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Dense, GRU\n",
    "from tensorflow.keras import Model\n",
    "#from tensorflow.data.Dataset import from_tensor_slice\n",
    "from tensorflow.train import AdamOptimizer\n",
    "from tensorflow.losses import sparse_softmax_cross_entropy\n",
    "import numpy as np\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'corpus.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(path, encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "text = re.sub('[^a-z\\n]', ' ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want the text backward so as to predict the previous letters instead of the next. The idea is to feed the model with a rhyme and get a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = text[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique = sorted(set(text))  # unique contains all the unique characters in the corpus\n",
    "\n",
    "char2idx = {u:i for i, u in enumerate(unique)}  # maps characters to indexes\n",
    "idx2char = {i:u for i, u in enumerate(unique)}  # maps indexes to characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_length = 100  # Maximum length sentence we want per input in the network\n",
    "vocab_size = len(unique)\n",
    "embedding_dim = 128  # number of 'meaningful' features to learn. Ex: ['queen', 'king', 'man', 'woman'] has a least 2 embedding dimension: royalty and gender.\n",
    "units = 512  # In keras: number of output of a sequence. In short it rem\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_text = []\n",
    "target_text = []\n",
    "\n",
    "for f in range(0, len(text) - max_length, max_length):\n",
    "    inps = text[f : f + max_length]\n",
    "    targ = text[f + 1 : f + 1 + max_length]\n",
    "    input_text.append([char2idx[i] for i in inps])\n",
    "    target_text.append([char2idx[t] for t in targ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input_text, target_text)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaination\n",
    "\n",
    "In fact, the algorithm does not learn which characters comes next. It analyzes sequences of characters as inputs (ex: 'abcd'), and predicts sequences as outputs (ex: 'bcde').\n",
    "\n",
    "Why?\n",
    "\n",
    "During the training phase, it learns more that just the next character. It updates weights for each characters from the input sequence to the output sequence.\n",
    "\n",
    "> Consider the sequences 'abcd', 'bcde', 'cdef', 'defg', the letter \"d\" is given different weights that depend on the previous sequences\n",
    "\n",
    "The use of these updates helps predicting better the next sequences and so on. So it learns the next character but also all the subsequent weights to better predict the next letter\n",
    "\n",
    "In our dataset, an example of input and target are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example of input:\n",
    "print('Given the following sequence: \\n\\n')\n",
    "print(''.join(idx2char[input_text[14][i]] for i in range(len(target_text[0]))))\n",
    "print('\\n\\n')\n",
    "print('the network has to learn that a correct continuation is: \\n')\n",
    "# example of output the algorithm has to learn\n",
    "print(''.join(idx2char[target_text[14][i]] for i in range(len(input_text[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "We build a model with:\n",
    "  * an embedding layer to prepare output to feed the GRU layer\n",
    "  * a GRU (Gated Recurrent Unit) layer\n",
    "  * a regular neural network layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, units, batch_size):\n",
    "    super(Model, self).__init__()\n",
    "    self.units = units\n",
    "    self.batch_sz = batch_size\n",
    "    self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = GRU(self.units, \n",
    "                   return_sequences=True, \n",
    "                   return_state=True, \n",
    "                   recurrent_activation='sigmoid', \n",
    "                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = Dense(vocab_size)\n",
    "        \n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, states = self.gru(x, initial_state=hidden)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "    x = self.fc(output)\n",
    "    return x, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(vocab_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we choose a regular AdamOptimizer, and a cross entropy loss funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_function(real, preds):\n",
    "    return sparse_softmax_cross_entropy(labels=real, logits=preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We train the model over 100 epoch (you can train it longer if you want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start = time.time()\n",
    "    hidden = model.reset_states()  # initializes the hidden state at the start of every epoch\n",
    "    \n",
    "    for (batch, (inp, target)) in enumerate(dataset):\n",
    "          with tf.GradientTape() as tape:\n",
    "              predictions, hidden = model(inp, hidden)  # predicts next letter given an input\n",
    "              target = tf.reshape(target, (-1, ))\n",
    "              loss = loss_function(target, predictions)  # compares the prediction with the real output\n",
    "\n",
    "          grads = tape.gradient(loss, model.variables)\n",
    "          optimizer.apply_gradients(zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())  # Gradient descent\n",
    "\n",
    "          if batch % 100 == 0:\n",
    "              print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, loss))\n",
    "    \n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, loss))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save our model\n",
    "\n",
    "We often want to save a model and use it later. Here is a way to do it. (some improvements can be made)\n",
    "\n",
    "First, put all parameters (hyperparameters and weights) in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {}\n",
    "parameters['max_length'] = max_length\n",
    "parameters['vocab_size'] = vocab_size\n",
    "parameters['embedding_dim'] = embedding_dim\n",
    "parameters['units'] = units\n",
    "parameters['BATCH_SIZE'] = BATCH_SIZE\n",
    "parameters['BUFFER_SIZE'] = BUFFER_SIZE\n",
    "parameters['embedding_weights'] = embedding_weights\n",
    "parameters['gru_weights'] = gru_weights\n",
    "parameters['fc_weights'] = fc_weights\n",
    "parameters['char2idx'] = char2idx\n",
    "parameters['idx2char'] = idx2char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, save the parameters, the weights for every layers we have trained (embedding, gru, fc) in a npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('model_poems', parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation\n",
    "\n",
    "We can now see how the model performs. \n",
    "\n",
    "A value we want to tune is the temperature. It tells how 'random' we want our predictions to be. The lowest the value, the more the prediction is random; giving nonsense. Greater values favorize the letter that is the most probable; creating a lot of repetitions.\n",
    "\n",
    "Feel free to change the rhymes and the temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rhymes = [' the child', ' be', ' me', ' wild']\n",
    "\n",
    "# We test different temperatures\n",
    "for temperature in [0.2, 0.5, 0.8, 0.9, 1.1]:\n",
    "    print('Temperature = {} \\n'.format(temperature))\n",
    "    text_generated = ''\n",
    "    \n",
    "    # Here we generate a new line given the last word (the rhyme)\n",
    "    for rhyme in rhymes:\n",
    "        num_generate = 150  # maximum number of characters to generate per line\n",
    "        start_string = text_generated + rhyme[::-1]  # remember we generate poems from the end to the start\n",
    "        input_eval = [char2idx[s] for s in start_string]  # converts start_string to numbers the model understands\n",
    "        input_eval = tf.expand_dims(input_eval, 0)  # \n",
    "        hidden = [tf.zeros((1, units))]  # initializes the hidden layer to zeros (there's maybe a better approach)\n",
    "        \n",
    "        b = True\n",
    "        c = 1\n",
    "        text_to_add = ''\n",
    "        tmp = start_string + ' '\n",
    "        while b == True:\n",
    "            # Add predicted letters\n",
    "            predictions, hidden = model(input_eval, hidden)  # predictions holds the probabily for each character to be most adequate continuation\n",
    "            predictions = predictions / temperature  # alters characters' probabilities to be picked (but keeps the order)\n",
    "            predicted_id = tf.multinomial(tf.exp(predictions), num_samples=1)[0][0].numpy()  # picks the next character for the generated text\n",
    "            input_eval = tf.expand_dims([predicted_id], 0)\n",
    "            tmp += idx2char[predicted_id]\n",
    "            text_to_add += idx2char[predicted_id]\n",
    "            \n",
    "            c += 1\n",
    "            \n",
    "            # Stop as soon as there is a line break\n",
    "            if idx2char[predicted_id] == '\\n' or c > num_generate:\n",
    "                text_generated += rhyme[::-1] + text_to_add \n",
    "                b = False\n",
    "    print('Poem : \\n')\n",
    "    print (text_generated[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "That's promising:\n",
    "* It can spell words correctly\n",
    "* There is some structure (line breaks).\n",
    "\n",
    "Harder-to-fix issue: Lines make little sense.\n",
    "\n",
    "Possible improvements:\n",
    "* further training\n",
    "* hyperparameters tuning\n",
    "* more consistent corpus (some word are truncated with a bit a slang)\n",
    "\n",
    "*Note: Punctunation was removed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
