{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perth Machine Learning Group Poem Generator\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The following code uses GRU to generate rhymes.\n",
    "\n",
    "In short, it observes sequences of rhymes, and infers the next rhyme.\n",
    "\n",
    "## The code\n",
    "\n",
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf  # version 1.9 or above\n",
    "tf.enable_eager_execution()  # Execution of code as it runs in the notebook. Normally, TensorFlow looks up the whole code before execution for efficiency.\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Dense, GRU\n",
    "from tensorflow.keras import Model\n",
    "#from tensorflow.data.Dataset import from_tensor_slice\n",
    "from tensorflow.train import AdamOptimizer\n",
    "from tensorflow.losses import sparse_softmax_cross_entropy\n",
    "import numpy as np\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'rhymes.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "text = re.sub('[^a-z\\n]', ' ', text)\n",
    "text = text.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = sorted(set(text))  # contains all the unique words in the corpus of rhymes\n",
    "\n",
    "word2idx = {u:i for i, u in enumerate(unique)}  # maps words to indexes\n",
    "idx2word = {i:u for i, u in enumerate(unique)}  # maps indexes to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100  # Maximum length sentence we want per input in the network\n",
    "vocab_size = len(unique)\n",
    "embedding_dim = 128  # number of 'meaningful' features to learn. Ex: ['queen', 'king', 'man', 'woman'] has a least 2 embedding dimension: royalty and gender.\n",
    "units = 512  # In keras: number of output of a sequence. In short it rem\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = []\n",
    "target_text = []\n",
    "\n",
    "for f in range(0, len(text) - max_length, max_length):\n",
    "    inps = text[f : f + max_length]\n",
    "    targ = text[f + 1 : f + 1 + max_length]\n",
    "    input_text.append([word2idx[i] for i in inps])\n",
    "    target_text.append([word2idx[t] for t in targ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input_text, target_text)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaination\n",
    "\n",
    "In fact, the algorithm does not learn which characters comes next. It analyzes sequences of characters as inputs (ex: 'abcd'), and predicts sequences as outputs (ex: 'bcde').\n",
    "\n",
    "Why?\n",
    "\n",
    "During the training phase, it learns more that just the next character. It updates weights for each characters from the input sequence to the output sequence.\n",
    "\n",
    "> Consider the sequences 'abcd', 'bcde', 'cdef', 'defg', the letter \"d\" is given different weights that depend on the previous sequences\n",
    "\n",
    "The use of these updates helps predicting better the next sequences and so on. So it learns the next character but also all the subsequent weights to better predict the next letter\n",
    "\n",
    "In our dataset, an example of input and target are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of input:\n",
    "print('Given the following sequence: \\n\\n')\n",
    "print([idx2word[input_text[15][i]] for i in range(len(target_text[0]))])\n",
    "print('\\n\\n')\n",
    "print('the network has to learn that a correct continuation is: \\n')\n",
    "# example of output the algorithm has to learn\n",
    "print([idx2word[target_text[15][i]] for i in range(len(input_text[0]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "We build a model with:\n",
    "  * an embedding layer to prepare output to feed the GRU layer\n",
    "  * a GRU (Gated Recurrent Unit) layer\n",
    "  * a regular neural network layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, units, batch_size):\n",
    "    super(Model, self).__init__()\n",
    "    self.units = units\n",
    "    self.batch_sz = batch_size\n",
    "    self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = GRU(self.units, \n",
    "                   return_sequences=True, \n",
    "                   return_state=True, \n",
    "                   recurrent_activation='sigmoid', \n",
    "                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = Dense(vocab_size)\n",
    "        \n",
    "  def call(self, x, hidden):\n",
    "    '''\n",
    "    Predicts an output given x\n",
    "    This function will be used for gradient descent during the training phase\n",
    "    '''\n",
    "    x = self.embedding(x)\n",
    "    output, states = self.gru(x, initial_state=hidden)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "    x = self.fc(output)\n",
    "    return x, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(vocab_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we choose a regular AdamOptimizer, and a cross entropy loss funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, preds):\n",
    "    return sparse_softmax_cross_entropy(labels=real, logits=preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We train the model over 100 epoch (you can train it longer if you want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start = time.time()\n",
    "    hidden = model.reset_states()  # initializes the hidden state at the start of every epoch\n",
    "    \n",
    "    for (batch, (inp, target)) in enumerate(dataset):\n",
    "          with tf.GradientTape() as tape:\n",
    "              predictions, hidden = model(inp, hidden)  # predicts next letter given an input\n",
    "              target = tf.reshape(target, (-1, ))\n",
    "              loss = loss_function(target, predictions)  # compares the prediction with the real output\n",
    "\n",
    "          grads = tape.gradient(loss, model.variables)\n",
    "          optimizer.apply_gradients(zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())  # Gradient descent\n",
    "\n",
    "          if batch % 100 == 0:\n",
    "              print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, loss))\n",
    "    \n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, loss))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save our model\n",
    "\n",
    "We often want to save a model and use it later. Here is a way to do it. (some improvements can be made)\n",
    "\n",
    "First, put all hyperparameters in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {}\n",
    "hyperparameters['max_length'] = max_length\n",
    "hyperparameters['vocab_size'] = vocab_size\n",
    "hyperparameters['embedding_dim'] = embedding_dim\n",
    "hyperparameters['units'] = units\n",
    "hyperparameters['BATCH_SIZE'] = BATCH_SIZE\n",
    "hyperparameters['BUFFER_SIZE'] = BUFFER_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, save the hyperparameters, the weights for every layers we have trained (embedding, gru, fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('hyperparameters_rhymes', hyperparameters)\n",
    "np.save('embedding_weights_rhymes', model.embedding.get_weights())\n",
    "np.save('gru_weights_rhymes', model.gru.get_weights())\n",
    "np.save('fc_weights_rhymes', model.fc.get_weights())\n",
    "np.save('word2idx_rhymes', word2idx)\n",
    "np.save('idx2word_rhymes', idx2word) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation\n",
    "\n",
    "We can now see how the model performs. \n",
    "\n",
    "A value we want to tune is the temperature. It tells how 'random' we want our predictions to be. The lowest the value, the more the prediction is random; giving nonsense. Greater values favorize the letter that is the most probable; creating a lot of repetitions.\n",
    "\n",
    "Feel free to change the rhymes and the temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_generate = 100  # number of characters to generate\n",
    "start_string = ['fell', 'vain', 'well', 'tree', 'fell', 'leave', 'me', 'above', 'melody']  # beginning of the generated text. TODO: try start_string = ' '\n",
    "\n",
    "input_eval = [word2idx[s] for s in start_string]  # converts start_string to numbers the model understands\n",
    "input_eval = tf.expand_dims(input_eval, 0)  # \n",
    "\n",
    "text_generated = []\n",
    "\n",
    "temperature = 0.0001# the greater, the closer to an observation in the corpus\n",
    "\n",
    "hidden = [tf.zeros((1, units))]\n",
    "for i in range(num_generate):\n",
    "    predictions, hidden = model(input_eval, hidden)  # predictions holds the probabily for each character to be most adequate continuation\n",
    "   \n",
    "    predictions = predictions / temperature  # alters characters' probabilities to be picked (but keeps the order)\n",
    "    predicted_id = tf.multinomial(tf.exp(predictions), num_samples=1)[0][0].numpy()  # picks the next character for the generated text\n",
    "    input_eval = tf.expand_dims([predicted_id], 0)\n",
    "    text_generated += [idx2word[predicted_id]]\n",
    "\n",
    "print (start_string + text_generated)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "That's promising. There are some interesting properties depending on how 'low' the temperature is (the meaning 'low' depend from a model to another)\n",
    "* Very low temperature may lead to prose\n",
    "* Low temperature can come up with weak rhymes by chance\n",
    "* high temperature lead to loops, some repetition of words. That could be an effect somebody looks for\n",
    "\n",
    "Hard-to-fix issue: rare words are really rare\n",
    "\n",
    "Possible improvements:\n",
    "* train at a character level?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
